{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671464db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e20fdbf49b42af96fe029811234c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cc25fba8fe4cf7bbff5e39bbcd4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Initializing...', description='Status:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reference image for case\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "\n",
    "OBJECT_IMAGES_PATH = \"dataset/\"\n",
    "DETECTION_THRESHOLD = 0.6   \n",
    "APPROACH_SPEED = 0.2  \n",
    "TURN_SPEED = 0.15   \n",
    "CENTER_THRESHOLD = 30 \n",
    "\n",
    "\n",
    "robot = Robot()\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "status_widget = widgets.Text(value='Initializing...', description='Status:')\n",
    "display(image_widget, status_widget)\n",
    "\n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "object_features = {}\n",
    "\n",
    "def load_reference_images():\n",
    "    status_widget.value = \"Loading reference images...\"\n",
    "    \n",
    "    if not os.path.exists(OBJECT_IMAGES_PATH):\n",
    "        status_widget.value = f\"Error: directory {OBJECT_IMAGES_PATH} not found\"\n",
    "        return False\n",
    "    \n",
    "    object_files = [f for f in os.listdir(OBJECT_IMAGES_PATH) if f.endswith('.jpg')]\n",
    "    \n",
    "    if not object_files:\n",
    "        status_widget.value = \"Error: No jpg files found in dataset directory\"\n",
    "        return False\n",
    "    \n",
    "    for file in object_files:\n",
    "        name = file.split('.')[0]\n",
    "        img_path = os.path.join(OBJECT_IMAGES_PATH, file)\n",
    "        \n",
    "        try:\n",
    "            image = PILImage.open(img_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = feature_extractor(image_tensor)\n",
    "                features = features.view(features.size(0), -1)\n",
    "                object_features[name] = features\n",
    "                \n",
    "            print(f\"Loaded reference image for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    \n",
    "    status_widget.value = f\"Loaded {len(object_features)} reference objects\"\n",
    "    return len(object_features) > 0\n",
    "\n",
    "def cosine_similarity(t1, t2):\n",
    "    t1_norm = torch.nn.functional.normalize(t1, p=2, dim=1)\n",
    "    t2_norm = torch.nn.functional.normalize(t2, p=2, dim=1)\n",
    "    return torch.mm(t1_norm, t2_norm.transpose(0, 1)).item()\n",
    "\n",
    "\n",
    "def identify_object():\n",
    "    frame = camera.value\n",
    "    if frame is None:\n",
    "        return None, 0.0\n",
    "        \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = PILImage.fromarray(frame_rgb)\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img_tensor)\n",
    "        features = features.view(features.size(0), -1) \n",
    " \n",
    "    similarities = {}\n",
    "    for name, ref_features in object_features.items():\n",
    "        sim = cosine_similarity(features, ref_features)\n",
    "        similarities[name] = sim\n",
    "\n",
    "    if similarities:\n",
    "        best_match = max(similarities.items(), key=lambda x: x[1])\n",
    "        if best_match[1] > DETECTION_THRESHOLD:\n",
    "            return best_match\n",
    "    \n",
    "    return None, 0.0\n",
    "\n",
    "def detect_object_position(frame, object_name):\n",
    "    h, w = frame.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    \n",
    "    try:\n",
    "        ref_path = os.path.join(OBJECT_IMAGES_PATH, f\"{object_name}.jpg\")\n",
    "        template = cv2.imread(ref_path)\n",
    "        template = cv2.resize(template, (w // 4, h // 4))  # Resize template\n",
    "        \n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        result = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        \n",
    "        th, tw = template_gray.shape\n",
    "        top_left = max_loc\n",
    "        obj_center_x = top_left[0] + tw // 2\n",
    "        obj_center_y = top_left[1] + th // 2\n",
    "        \n",
    "        vis_frame = frame.copy()\n",
    "        cv2.rectangle(vis_frame, top_left, (top_left[0] + tw, top_left[1] + th), (0, 255, 0), 2)\n",
    "        cv2.circle(vis_frame, (obj_center_x, obj_center_y), 5, (0, 0, 255), -1)\n",
    "        cv2.line(vis_frame, (center_x, center_y), (obj_center_x, obj_center_y), (255, 0, 0), 2)\n",
    "        cv2.putText(vis_frame, f\"{object_name}: {max_val:.2f}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        x_offset = obj_center_x - center_x\n",
    "        \n",
    "        image_widget.value = bgr8_to_jpeg(vis_frame)\n",
    "        \n",
    "        return (x_offset, max_val)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in position detection: {e}\")\n",
    "        return (0, 0)\n",
    "\n",
    "def detect_and_approach():\n",
    "    try:\n",
    "        if not load_reference_images():\n",
    "            print(\"Failed to load reference images. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        status_widget.value = \"Starting object detection...\"\n",
    "        \n",
    "        while True:\n",
    "            object_name, confidence = identify_object()\n",
    "            \n",
    "            if object_name:\n",
    "                status_widget.value = f\"Detected {object_name} ({confidence:.2f})\"\n",
    "                \n",
    "                frame = camera.value\n",
    "                if frame is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get object position\n",
    "                x_offset, match_conf = detect_object_position(frame, object_name)\n",
    "                \n",
    "                if abs(x_offset) < CENTER_THRESHOLD:\n",
    "                    status_widget.value = f\"Moving toward {object_name}\"\n",
    "                    robot.forward(APPROACH_SPEED)\n",
    "                    time.sleep(0.5)\n",
    "                elif x_offset > 0:\n",
    "                    status_widget.value = f\"Turning right to center {object_name}\"\n",
    "                    robot.right(TURN_SPEED)\n",
    "                    time.sleep(0.2)\n",
    "                else:\n",
    "                    status_widget.value = f\"Turning left to center {object_name}\"\n",
    "                    robot.left(TURN_SPEED)\n",
    "                    time.sleep(0.2)\n",
    "                \n",
    "                robot.stop()\n",
    "                time.sleep(0.1)\n",
    "            else:\n",
    "                status_widget.value = \"No objects detected, scanning...\"\n",
    "                robot.left(TURN_SPEED * 0.8)\n",
    "                time.sleep(0.3)\n",
    "                robot.stop()\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        status_widget.value = \"Stopped by user\"\n",
    "    except Exception as e:\n",
    "        status_widget.value = f\"Error: {e}\"\n",
    "    finally:\n",
    "        robot.stop()\n",
    "        camera.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_and_approach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903951e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
