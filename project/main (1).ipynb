{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bc3f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeeb45835a0447baa7b3d508a2dc2b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='500', width='500')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ef60ea95104054a18ef3fbe2d21b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, description='Start / Stop')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded!\n",
      "Run the function execute_detection_and_navigation() to start the navigation\n"
     ]
    }
   ],
   "source": [
    "# JetBot Object Detection and Navigation\n",
    "# This notebook detects bottles and cases, navigates toward them, and takes photos upon reaching them\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "from jetbot import Robot, Camera\n",
    "\n",
    "# Create output directory for photos\n",
    "OUTPUT_DIR = 'captured_photos'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Define the target classes to detect (from COCO dataset)\n",
    "# COCO class indices: bottle=39, suitcase=28\n",
    "TARGET_CLASSES = {\n",
    "    'bottle': 39,\n",
    "    'case': 28,\n",
    "    'phone': 67,\n",
    "    'book': 73,\n",
    "    'cup': 41,\n",
    "    'keyboard': 66,\n",
    "    'mouse': 64,\n",
    "    'backpack': 24,\n",
    "    'chair': 56,\n",
    "    'apple': 47,\n",
    "    'remote': 65,\n",
    "    'laptop': 63\n",
    "}\n",
    "\n",
    "# Initialize the JetBot\n",
    "robot = Robot()\n",
    "\n",
    "# Initialize the camera\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "# Function to load the pre-trained object detection model (SSD MobileNet)\n",
    "def load_model():\n",
    "    # Load pre-trained SSD MobileNet model from torchvision\n",
    "    model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "    model.eval().cuda()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the image for the model\n",
    "def preprocess(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "# Function to detect objects in the image\n",
    "def detect_objects(model, image, confidence_threshold=0.5):\n",
    "    # Convert image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    input_tensor = preprocess(image_rgb)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "    \n",
    "    # Extract detection results\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter detections based on confidence threshold\n",
    "    mask = scores > confidence_threshold\n",
    "    boxes = boxes[mask]\n",
    "    labels = labels[mask]\n",
    "    scores = scores[mask]\n",
    "    \n",
    "    return boxes, labels, scores\n",
    "\n",
    "# Function to find target objects in the detections\n",
    "def find_target_objects(labels, boxes, scores):\n",
    "    target_indices = {}\n",
    "    target_boxes = {}\n",
    "    target_scores = {}\n",
    "    \n",
    "    for target_name, target_class in TARGET_CLASSES.items():\n",
    "        # Find indices where the label matches the target class\n",
    "        indices = np.where(labels == target_class)[0]\n",
    "        if len(indices) > 0:\n",
    "            # Get the index of the detection with the highest confidence score\n",
    "            best_idx = indices[np.argmax(scores[indices])]\n",
    "            target_indices[target_name] = best_idx\n",
    "            target_boxes[target_name] = boxes[best_idx]\n",
    "            target_scores[target_name] = scores[best_idx]\n",
    "    \n",
    "    return target_indices, target_boxes, target_scores\n",
    "\n",
    "# Function to calculate the center of a bounding box\n",
    "def get_box_center(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "# Function to calculate the area of a bounding box\n",
    "def get_box_area(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    return (x2 - x1) * (y2 - y1)\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_boxes(image, boxes, labels, scores):\n",
    "    # Create a copy of the image\n",
    "    image_with_boxes = image.copy()\n",
    "    \n",
    "    # Define colors for each class (in BGR)\n",
    "    colors = {\n",
    "        TARGET_CLASSES['bottle']: (0, 255, 0),  # Green for bottle\n",
    "        TARGET_CLASSES['case']: (0, 0, 255)     # Red for case\n",
    "    }\n",
    "    \n",
    "    # Draw each bounding box\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        color = colors.get(label, (255, 0, 0))  # Default blue for other objects\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Add label and score\n",
    "        label_text = f\"Class: {label}, {score:.2f}\"\n",
    "        for target_name, target_class in TARGET_CLASSES.items():\n",
    "            if label == target_class:\n",
    "                label_text = f\"{target_name}: {score:.2f}\"\n",
    "                break\n",
    "                \n",
    "        cv2.putText(image_with_boxes, label_text, (x1, y1 - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "# Function to navigate towards a target\n",
    "def navigate_to_target(robot, center_x, image_width, target_area, full_image_area, stop_threshold=0.4):\n",
    "    # Calculate the relative position of the target in the frame\n",
    "    image_center_x = image_width / 2\n",
    "    offset = center_x - image_center_x\n",
    "    \n",
    "    # Calculate the normalized offset (-1 to 1)\n",
    "    normalized_offset = offset / image_center_x\n",
    "    \n",
    "    # Calculate area ratio to determine distance\n",
    "    area_ratio = target_area / full_image_area\n",
    "    \n",
    "    # Determine if we're close enough to stop\n",
    "    if area_ratio > stop_threshold:\n",
    "        # We've reached the target, stop the robot\n",
    "        robot.stop()\n",
    "        return True\n",
    "    \n",
    "    # Set motor speeds based on the target position\n",
    "    # The further the target is from the center, the more we turn\n",
    "    # The larger the target appears, the slower we move\n",
    "    \n",
    "    # Base speed decreases as we get closer to the target\n",
    "    base_speed = max(0.3, 0.7 - area_ratio)\n",
    "    \n",
    "    # Calculate left and right motor speeds\n",
    "    left_speed = base_speed\n",
    "    right_speed = base_speed\n",
    "    \n",
    "    # Adjust speeds based on the target's horizontal position\n",
    "    if normalized_offset < -0.1:  # Target is to the left\n",
    "        # Turn left by decreasing right motor speed\n",
    "        right_speed += min(0.4, abs(normalized_offset) * 0.8)\n",
    "        left_speed -= min(0.3, abs(normalized_offset) * 0.4)\n",
    "    elif normalized_offset > 0.1:  # Target is to the right\n",
    "        # Turn right by decreasing left motor speed\n",
    "        left_speed += min(0.4, normalized_offset * 0.8)\n",
    "        right_speed -= min(0.3, normalized_offset * 0.4)\n",
    "    \n",
    "    # Ensure speeds are within valid range\n",
    "    left_speed = max(0.0, min(1.0, left_speed))\n",
    "    right_speed = max(0.0, min(1.0, right_speed))\n",
    "    \n",
    "    # Apply motor speeds\n",
    "    robot.left_motor.value = left_speed\n",
    "    robot.right_motor.value = right_speed\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Function to take a photo and save it\n",
    "def take_photo(image, object_name):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{OUTPUT_DIR}/{object_name}_{timestamp}.jpg\"\n",
    "    cv2.imwrite(filename, image)\n",
    "    print(f\"Photo saved as {filename}\")\n",
    "\n",
    "# Create display widgets\n",
    "image_widget = ipywidgets.Image(format='jpeg', width=500, height=500)\n",
    "display(image_widget)\n",
    "\n",
    "# Button to start and stop the robot\n",
    "run_button = ipywidgets.ToggleButton(description='Start / Stop')\n",
    "display(run_button)\n",
    "\n",
    "# Load the object detection model\n",
    "print(\"Loading model...\")\n",
    "model = load_model()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Main execution loop\n",
    "def execute_detection_and_navigation():\n",
    "    running = False\n",
    "    target_reached = {target: False for target in TARGET_CLASSES}\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Check if the button state has changed\n",
    "            if run_button.value != running:\n",
    "                running = run_button.value\n",
    "                if not running:\n",
    "                    robot.stop()\n",
    "                    print(\"Stopped\")\n",
    "            \n",
    "            # Capture image from camera\n",
    "            image = camera.value\n",
    "            \n",
    "            if image is not None:\n",
    "                # Convert the image to BGR format for OpenCV processing\n",
    "                image_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "                height, width = image_bgr.shape[:2]\n",
    "                full_image_area = height * width\n",
    "                \n",
    "                # Only perform detection if the robot is running\n",
    "                if running:\n",
    "                    # Detect objects in the image\n",
    "                    boxes, labels, scores = detect_objects(model, image_bgr)\n",
    "                    \n",
    "                    # Find target objects\n",
    "                    target_indices, target_boxes, target_scores = find_target_objects(labels, boxes, scores)\n",
    "                    \n",
    "                    # Check if any target object is found\n",
    "                    if target_boxes:\n",
    "                        # Select the target with the highest confidence score\n",
    "                        selected_target = max(target_scores.items(), key=lambda x: x[1])[0]\n",
    "                        selected_box = target_boxes[selected_target]\n",
    "                        \n",
    "                        # Calculate the center of the selected target\n",
    "                        center_x, _ = get_box_center(selected_box)\n",
    "                        \n",
    "                        # Calculate the area of the target\n",
    "                        target_area = get_box_area(selected_box)\n",
    "                        \n",
    "                        # Navigate towards the target\n",
    "                        reached = navigate_to_target(robot, center_x, width, target_area, full_image_area)\n",
    "                        \n",
    "                        # If we've reached the target and haven't taken a photo yet\n",
    "                        if reached and not target_reached[selected_target]:\n",
    "                            take_photo(image_bgr, selected_target)\n",
    "                            target_reached[selected_target] = True\n",
    "                            print(f\"Reached {selected_target}!\")\n",
    "                            # Wait a moment to avoid taking multiple photos\n",
    "                            time.sleep(2)\n",
    "                        elif not reached:\n",
    "                            # Reset the target_reached status if we move away from the target\n",
    "                            target_reached[selected_target] = False\n",
    "                    else:\n",
    "                        # No target found, stop the robot\n",
    "                        robot.stop()\n",
    "                \n",
    "                # Always draw boxes for better visualization\n",
    "                if 'boxes' in locals() and len(boxes) > 0:\n",
    "                    image_with_boxes = draw_boxes(image_bgr, boxes, labels, scores)\n",
    "                else:\n",
    "                    image_with_boxes = image_bgr\n",
    "                \n",
    "                # Convert back to RGB for display\n",
    "                image_rgb = cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Update the image widget\n",
    "                image_widget.value = cv2.imencode('.jpg', image_rgb)[1].tobytes()\n",
    "            \n",
    "            # Brief pause to prevent high CPU usage\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        robot.stop()\n",
    "        print(\"Program stopped\")\n",
    "    except Exception as e:\n",
    "        robot.stop()\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the main function in a separate cell using:\n",
    "# execute_detection_and_navigation()\n",
    "print(\"Run the function execute_detection_and_navigation() to start the navigation\")\n",
    "\n",
    "# Example usage:\n",
    "# execute_detection_and_navigation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9accc247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program stopped\n"
     ]
    }
   ],
   "source": [
    "execute_detection_and_navigation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952fdf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b841dcfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'PRIORITY_ORDER' is used prior to global declaration (<ipython-input-7-0a66a0ddaae7>, line 450)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0a66a0ddaae7>\"\u001b[0;36m, line \u001b[0;32m450\u001b[0m\n\u001b[0;31m    perform_patrol()\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'PRIORITY_ORDER' is used prior to global declaration\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# --- CONFIGURABLE PARAMETERS ---\n",
    "PRIORITY_ORDER = [\"bottle\", \"case\"]  # Define your own order here\n",
    "OBJECT_IMAGES_PATH = \"dataset/\"  # Folder containing bottle.jpg, case.jpg, bag.jpg\n",
    "SAVE_PATH = \"logs/\"              # Folder to save visit pictures and logs\n",
    "DETECTION_THRESHOLD = 0.6        # Minimum similarity threshold for object detection\n",
    "SCAN_SPEED = 0.3               # Speed for scanning rotation\n",
    "APPROACH_SPEED = 0.15            # Speed for approaching objects\n",
    "SCAN_STEP_TIME = 0.3             # Time between scan steps\n",
    "MAX_APPROACH_TIME = 5.0          # Maximum time to approach an object (seconds)\n",
    "CENTER_THRESHOLD = 30            # Pixels from center to consider object centered\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# --- SETUP ---\n",
    "robot = Robot()\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "# Create widgets for visualization\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "status_widget = widgets.Text(value='Initializing...', description='Status:')\n",
    "display(image_widget, status_widget)\n",
    "\n",
    "# Define the function to convert BGR to JPEG for display\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# Link camera to image widget\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# Load MobileNetV2 for feature extraction\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "# Remove the classifier to get features instead of classification\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Object name to feature vector dictionary\n",
    "object_features = {}\n",
    "\n",
    "# Load object reference images and extract features\n",
    "def load_reference_images():\n",
    "    status_widget.value = \"Loading reference images...\"\n",
    "    for name in PRIORITY_ORDER:\n",
    "        img_path = os.path.join(OBJECT_IMAGES_PATH, f\"{name}.jpg\")\n",
    "        try:\n",
    "            image = PILImage.open(img_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = feature_extractor(image_tensor)\n",
    "                features = features.view(features.size(0), -1)  # Flatten\n",
    "                object_features[name] = features\n",
    "                \n",
    "            print(f\"Loaded reference image for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    \n",
    "    status_widget.value = f\"Loaded {len(object_features)} reference objects\"\n",
    "\n",
    "# Cosine similarity for feature comparison\n",
    "def cosine_similarity(t1, t2):\n",
    "    t1_norm = torch.nn.functional.normalize(t1, p=2, dim=1)\n",
    "    t2_norm = torch.nn.functional.normalize(t2, p=2, dim=1)\n",
    "    return torch.mm(t1_norm, t2_norm.transpose(0, 1)).item()\n",
    "\n",
    "# Capture current frame and compare with reference objects\n",
    "def identify_object():\n",
    "    frame = camera.value\n",
    "    if frame is None:\n",
    "        return None, 0.0\n",
    "        \n",
    "    # Convert BGR to RGB (OpenCV uses BGR, PIL expects RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = PILImage.fromarray(frame_rgb)\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img_tensor)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "    \n",
    "    # Calculate similarity with each reference object\n",
    "    similarities = {}\n",
    "    for name, ref_features in object_features.items():\n",
    "        sim = cosine_similarity(features, ref_features)\n",
    "        similarities[name] = sim\n",
    "    \n",
    "    # Get the most similar object above threshold\n",
    "    best_match = max(similarities.items(), key=lambda x: x[1])\n",
    "    if best_match[1] > DETECTION_THRESHOLD:\n",
    "        return best_match\n",
    "    else:\n",
    "        return None, 0.0\n",
    "\n",
    "# Detect object in frame and get bounding box (placeholder using template matching)\n",
    "def detect_object_position(frame, object_name):\n",
    "    h, w = frame.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    \n",
    "    # This is a simplified detection method using template matching\n",
    "    # In a real scenario, you might want to use a proper object detection model\n",
    "    try:\n",
    "        # Load the reference image\n",
    "        ref_path = os.path.join(OBJECT_IMAGES_PATH, f\"{object_name}.jpg\")\n",
    "        template = cv2.imread(ref_path)\n",
    "        template = cv2.resize(template, (w // 4, h // 4))  # Resize template\n",
    "        \n",
    "        # Convert both images to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform template matching\n",
    "        result = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        \n",
    "        # Calculate center of matched region\n",
    "        th, tw = template_gray.shape\n",
    "        top_left = max_loc\n",
    "        obj_center_x = top_left[0] + tw // 2\n",
    "        obj_center_y = top_left[1] + th // 2\n",
    "        \n",
    "        # Draw rectangle on visualization\n",
    "        vis_frame = frame.copy()\n",
    "        cv2.rectangle(vis_frame, top_left, (top_left[0] + tw, top_left[1] + th), (0, 255, 0), 2)\n",
    "        cv2.circle(vis_frame, (obj_center_x, obj_center_y), 5, (0, 0, 255), -1)\n",
    "        cv2.putText(vis_frame, f\"{object_name}: {max_val:.2f}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Update display\n",
    "        image_widget.value = bgr8_to_jpeg(vis_frame)\n",
    "        \n",
    "        # Calculate offset from center (normalized to -1 to 1)\n",
    "        x_offset = (obj_center_x - center_x) / (w / 2)\n",
    "        y_offset = (obj_center_y - center_y) / (h / 2)\n",
    "        \n",
    "        return (x_offset, y_offset, max_val)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in position detection: {e}\")\n",
    "        return (0, 0, 0)\n",
    "\n",
    "# Scan environment for objects\n",
    "def scan_objects():\n",
    "    status_widget.value = \"Scanning for objects...\"\n",
    "    detected = set()\n",
    "    detected_with_conf = {}\n",
    "    \n",
    "    # Do a full 360° scan in smaller steps\n",
    "    for _ in range(24):  # 15° steps for full rotation\n",
    "        robot.left(SCAN_SPEED)\n",
    "        time.sleep(SCAN_STEP_TIME)\n",
    "        robot.stop()\n",
    "        time.sleep(0.2)  # Stabilize camera\n",
    "        \n",
    "        # Identify object\n",
    "        obj, confidence = identify_object()\n",
    "        if obj:\n",
    "            detected.add(obj)\n",
    "            detected_with_conf[obj] = max(confidence, detected_with_conf.get(obj, 0))\n",
    "            status_widget.value = f\"Found {obj} (conf: {confidence:.2f})\"\n",
    "            \n",
    "        # Display progress\n",
    "        found_str = \", \".join([f\"{o}:{c:.2f}\" for o, c in detected_with_conf.items()])\n",
    "        print(f\"Scan progress: {found_str}\")\n",
    "        \n",
    "        # If all objects found, break early\n",
    "        if all(obj in detected for obj in PRIORITY_ORDER):\n",
    "            break\n",
    "    \n",
    "    robot.stop()\n",
    "    status_widget.value = f\"Scan complete. Found: {', '.join(detected)}\"\n",
    "    return detected\n",
    "\n",
    "# Approach an object\n",
    "def go_to_object(target):\n",
    "    status_widget.value = f\"Moving to {target}...\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Try to approach the object\n",
    "    while time.time() - start_time < MAX_APPROACH_TIME:\n",
    "        frame = camera.value\n",
    "        if frame is None:\n",
    "            continue\n",
    "            \n",
    "        # Check if we're seeing the target object\n",
    "        obj, confidence = identify_object()\n",
    "        \n",
    "        if obj == target and confidence > DETECTION_THRESHOLD:\n",
    "            # Get position and calculate steering adjustment\n",
    "            x_offset, y_offset, match_conf = detect_object_position(frame, target)\n",
    "            \n",
    "            # If we're very close (object fills much of the frame or is centered)\n",
    "            if abs(x_offset) < (CENTER_THRESHOLD / 112):  # 112 is half of 224\n",
    "                # Go straight if centered\n",
    "                robot.forward(APPROACH_SPEED)\n",
    "                \n",
    "                # If we're very close, stop and capture\n",
    "                if match_conf > 0.8:  # High confidence means we're close\n",
    "                    robot.stop()\n",
    "                    status_widget.value = f\"Reached {target}!\"\n",
    "                    time.sleep(0.5)  # Wait for camera to stabilize\n",
    "                    break\n",
    "            elif x_offset > 0:\n",
    "                # Object is to the right, turn right\n",
    "                robot.right(APPROACH_SPEED * 0.8)\n",
    "                time.sleep(0.1)\n",
    "                robot.stop()\n",
    "            else:\n",
    "                # Object is to the left, turn left\n",
    "                robot.left(APPROACH_SPEED * 0.8)\n",
    "                time.sleep(0.1)\n",
    "                robot.stop()\n",
    "                \n",
    "            # Move forward a bit\n",
    "            robot.forward(APPROACH_SPEED)\n",
    "            time.sleep(0.3)\n",
    "            robot.stop()\n",
    "        else:\n",
    "            # If we lost the object, do a small scan to find it\n",
    "            robot.left(SCAN_SPEED * 0.5)\n",
    "            time.sleep(0.2)\n",
    "            robot.stop()\n",
    "    \n",
    "    # Stop the robot\n",
    "    robot.stop()\n",
    "    status_widget.value = f\"Taking picture of {target}\"\n",
    "    \n",
    "    # Capture image\n",
    "    capture_image(target)\n",
    "\n",
    "# Turn 180°\n",
    "def turn_180():\n",
    "    status_widget.value = \"Turning 180°\"\n",
    "    robot.left(SCAN_SPEED)\n",
    "    time.sleep(3.0)  # Adjust time for smoother 180° turn\n",
    "    robot.stop()\n",
    "\n",
    "# Save image capture\n",
    "def capture_image(obj_name):\n",
    "    # Wait for camera to stabilize\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    frame = camera.value\n",
    "    if frame is None:\n",
    "        print(\"Error: Could not capture frame\")\n",
    "        return\n",
    "        \n",
    "    # Save the raw image\n",
    "    timestamp = int(time.time())\n",
    "    path = os.path.join(SAVE_PATH, f\"{obj_name}_{timestamp}.jpg\")\n",
    "    \n",
    "    # Convert BGR to RGB for PIL\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = PILImage.fromarray(frame_rgb)\n",
    "    img.save(path)\n",
    "    \n",
    "    print(f\"Saved image at {path}\")\n",
    "    status_widget.value = f\"Saved image of {obj_name}\"\n",
    "\n",
    "# Visit all objects in priority\n",
    "def perform_patrol():\n",
    "    log = []\n",
    "    total_energy = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for obj in PRIORITY_ORDER:\n",
    "        t0 = time.time()\n",
    "        status_widget.value = f\"Looking for {obj}\"\n",
    "        \n",
    "        # Scan until we find the current object\n",
    "        found = False\n",
    "        for _ in range(12):  # Try up to 12 scan segments\n",
    "            robot.left(SCAN_SPEED)\n",
    "            time.sleep(SCAN_STEP_TIME)\n",
    "            robot.stop()\n",
    "            \n",
    "            obj_found, confidence = identify_object()\n",
    "            if obj_found == obj:\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if found:\n",
    "            go_to_object(obj)\n",
    "            turn_180()  # After reaching object, turn 180°\n",
    "        else:\n",
    "            status_widget.value = f\"Could not find {obj}\"\n",
    "            \n",
    "        t1 = time.time()\n",
    "        duration = t1 - t0\n",
    "        energy = duration * 2  # Simplistic energy estimate\n",
    "        total_time += duration\n",
    "        total_energy += energy\n",
    "        log.append((obj, round(duration, 2), round(energy, 2)))\n",
    "\n",
    "    # Final stop\n",
    "    robot.stop()\n",
    "\n",
    "    # Print log\n",
    "    print(\"\\n--- PATROL LOG ---\")\n",
    "    for entry in log:\n",
    "        print(f\"Visited {entry[0]} | Time: {entry[1]}s | Energy: {entry[2]}\")\n",
    "    print(f\"\\nTotal Time: {round(total_time, 2)}s | Total Energy: {round(total_energy, 2)}\")\n",
    "    status_widget.value = \"Patrol complete!\"\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load reference images first\n",
    "        load_reference_images()\n",
    "        \n",
    "        print(\"Starting patrol...\")\n",
    "        status_widget.value = \"Starting initial scan...\"\n",
    "        \n",
    "        # Scan for objects\n",
    "        detected = scan_objects()\n",
    "        \n",
    "        # Check if all target objects were found\n",
    "        missing = set(PRIORITY_ORDER) - detected\n",
    "        if not missing:\n",
    "            status_widget.value = \"All objects found. Starting patrol.\"\n",
    "            perform_patrol()\n",
    "        else:\n",
    "            status_widget.value = f\"Missing objects: {', '.join(missing)}. Aborting.\"\n",
    "            print(\"Not all target objects found. Aborting mission.\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Mission aborted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Always stop the robot and camera when done\n",
    "        robot.stop()\n",
    "        camera.stop()\n",
    "        status_widget.value = \"Mission complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12486fb33b0a4757a2fbd41f20a237b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bd4221efbe4a1b9c3dcd6fb89bd618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Initializing...', description='Status:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reference image for case\n",
      "Loaded reference image for bottle\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from jetbot import Robot\n",
    "from jetbot import Camera\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# --- CONFIGURABLE PARAMETERS ---\n",
    "OBJECT_IMAGES_PATH = \"dataset/\"  # Folder containing reference object images\n",
    "DETECTION_THRESHOLD = 0.6        # Minimum similarity threshold for object detection\n",
    "APPROACH_SPEED = 0.2             # Speed for approaching objects\n",
    "TURN_SPEED = 0.15                # Speed for turning when object not centered\n",
    "CENTER_THRESHOLD = 30            # Pixels from center to consider object centered\n",
    "\n",
    "# --- SETUP ---\n",
    "robot = Robot()\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "# Create widgets for visualization\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "status_widget = widgets.Text(value='Initializing...', description='Status:')\n",
    "display(image_widget, status_widget)\n",
    "\n",
    "# Define the function to convert BGR to JPEG for display\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# Link camera to image widget\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# Load MobileNetV2 for feature extraction\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "# Remove the classifier to get features instead of classification\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Object name to feature vector dictionary\n",
    "object_features = {}\n",
    "\n",
    "# Load object reference images and extract features\n",
    "def load_reference_images():\n",
    "    status_widget.value = \"Loading reference images...\"\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(OBJECT_IMAGES_PATH):\n",
    "        status_widget.value = f\"Error: directory {OBJECT_IMAGES_PATH} not found\"\n",
    "        return False\n",
    "    \n",
    "    # Get all jpg files in the directory\n",
    "    object_files = [f for f in os.listdir(OBJECT_IMAGES_PATH) if f.endswith('.jpg')]\n",
    "    \n",
    "    if not object_files:\n",
    "        status_widget.value = \"Error: No jpg files found in dataset directory\"\n",
    "        return False\n",
    "    \n",
    "    for file in object_files:\n",
    "        # Get object name from filename (remove .jpg extension)\n",
    "        name = file.split('.')[0]\n",
    "        img_path = os.path.join(OBJECT_IMAGES_PATH, file)\n",
    "        \n",
    "        try:\n",
    "            image = PILImage.open(img_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = feature_extractor(image_tensor)\n",
    "                features = features.view(features.size(0), -1)  # Flatten\n",
    "                object_features[name] = features\n",
    "                \n",
    "            print(f\"Loaded reference image for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    \n",
    "    status_widget.value = f\"Loaded {len(object_features)} reference objects\"\n",
    "    return len(object_features) > 0\n",
    "\n",
    "# Cosine similarity for feature comparison\n",
    "def cosine_similarity(t1, t2):\n",
    "    t1_norm = torch.nn.functional.normalize(t1, p=2, dim=1)\n",
    "    t2_norm = torch.nn.functional.normalize(t2, p=2, dim=1)\n",
    "    return torch.mm(t1_norm, t2_norm.transpose(0, 1)).item()\n",
    "\n",
    "# Capture current frame and compare with reference objects\n",
    "def identify_object():\n",
    "    frame = camera.value\n",
    "    if frame is None:\n",
    "        return None, 0.0\n",
    "        \n",
    "    # Convert BGR to RGB (OpenCV uses BGR, PIL expects RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = PILImage.fromarray(frame_rgb)\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img_tensor)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "    \n",
    "    # Calculate similarity with each reference object\n",
    "    similarities = {}\n",
    "    for name, ref_features in object_features.items():\n",
    "        sim = cosine_similarity(features, ref_features)\n",
    "        similarities[name] = sim\n",
    "    \n",
    "    # Get the most similar object above threshold\n",
    "    if similarities:\n",
    "        best_match = max(similarities.items(), key=lambda x: x[1])\n",
    "        if best_match[1] > DETECTION_THRESHOLD:\n",
    "            return best_match\n",
    "    \n",
    "    return None, 0.0\n",
    "\n",
    "# Detect object in frame and get position information\n",
    "def detect_object_position(frame, object_name):\n",
    "    h, w = frame.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    \n",
    "    try:\n",
    "        # Load the reference image\n",
    "        ref_path = os.path.join(OBJECT_IMAGES_PATH, f\"{object_name}.jpg\")\n",
    "        template = cv2.imread(ref_path)\n",
    "        template = cv2.resize(template, (w // 4, h // 4))  # Resize template\n",
    "        \n",
    "        # Convert both images to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform template matching\n",
    "        result = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        \n",
    "        # Calculate center of matched region\n",
    "        th, tw = template_gray.shape\n",
    "        top_left = max_loc\n",
    "        obj_center_x = top_left[0] + tw // 2\n",
    "        obj_center_y = top_left[1] + th // 2\n",
    "        \n",
    "        # Draw rectangle and info on visualization\n",
    "        vis_frame = frame.copy()\n",
    "        cv2.rectangle(vis_frame, top_left, (top_left[0] + tw, top_left[1] + th), (0, 255, 0), 2)\n",
    "        cv2.circle(vis_frame, (obj_center_x, obj_center_y), 5, (0, 0, 255), -1)\n",
    "        cv2.line(vis_frame, (center_x, center_y), (obj_center_x, obj_center_y), (255, 0, 0), 2)\n",
    "        cv2.putText(vis_frame, f\"{object_name}: {max_val:.2f}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate offset from center (pixels)\n",
    "        x_offset = obj_center_x - center_x\n",
    "        \n",
    "        # Update display\n",
    "        image_widget.value = bgr8_to_jpeg(vis_frame)\n",
    "        \n",
    "        return (x_offset, max_val)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in position detection: {e}\")\n",
    "        return (0, 0)\n",
    "\n",
    "# Main function to detect and approach objects\n",
    "def detect_and_approach():\n",
    "    try:\n",
    "        # Load reference images first\n",
    "        if not load_reference_images():\n",
    "            print(\"Failed to load reference images. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        status_widget.value = \"Starting object detection...\"\n",
    "        \n",
    "        while True:\n",
    "            # Identify any object in the camera view\n",
    "            object_name, confidence = identify_object()\n",
    "            \n",
    "            if object_name:\n",
    "                status_widget.value = f\"Detected {object_name} ({confidence:.2f})\"\n",
    "                \n",
    "                # Get the current frame\n",
    "                frame = camera.value\n",
    "                if frame is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get object position\n",
    "                x_offset, match_conf = detect_object_position(frame, object_name)\n",
    "                \n",
    "                # Decide how to move based on object position\n",
    "                if abs(x_offset) < CENTER_THRESHOLD:\n",
    "                    # Object is centered, move forward\n",
    "                    status_widget.value = f\"Moving toward {object_name}\"\n",
    "                    robot.forward(APPROACH_SPEED)\n",
    "                    time.sleep(0.5)\n",
    "                elif x_offset > 0:\n",
    "                    # Object is to the right, turn right\n",
    "                    status_widget.value = f\"Turning right to center {object_name}\"\n",
    "                    robot.right(TURN_SPEED)\n",
    "                    time.sleep(0.2)\n",
    "                else:\n",
    "                    # Object is to the left, turn left\n",
    "                    status_widget.value = f\"Turning left to center {object_name}\"\n",
    "                    robot.left(TURN_SPEED)\n",
    "                    time.sleep(0.2)\n",
    "                \n",
    "                # Pause briefly\n",
    "                robot.stop()\n",
    "                time.sleep(0.1)\n",
    "            else:\n",
    "                # No object detected, slowly rotate to scan\n",
    "                status_widget.value = \"No objects detected, scanning...\"\n",
    "                robot.left(TURN_SPEED * 0.8)\n",
    "                time.sleep(0.3)\n",
    "                robot.stop()\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        status_widget.value = \"Stopped by user\"\n",
    "    except Exception as e:\n",
    "        status_widget.value = f\"Error: {e}\"\n",
    "    finally:\n",
    "        # Always stop the robot and camera when done\n",
    "        robot.stop()\n",
    "        camera.stop()\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    detect_and_approach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b0169d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4b88193b7124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mdetect_and_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-4b88193b7124>\u001b[0m in \u001b[0;36mdetect_and_act\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO_INSTANCE_CATEGORY_NAMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from jetbot import Camera, Robot\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image\n",
    "\n",
    "# Load pre-trained object detection model (Faster R-CNN)\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval().cuda()\n",
    "\n",
    "# COCO class labels\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '_background_', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
    "    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
    "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
    "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n",
    "    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n",
    "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "camera = Camera.instance(width=320, height=240)\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def detect_and_act():\n",
    "    try:\n",
    "        while True:\n",
    "            frame = camera.value\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            input_tensor = transform(img).unsqueeze(0).cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)[0]\n",
    "\n",
    "            labels = outputs['labels'].cpu().numpy()\n",
    "            scores = outputs['scores'].cpu().numpy()\n",
    "            boxes = outputs['boxes'].cpu().numpy()\n",
    "\n",
    "            # Default action\n",
    "            action = \"forward\"\n",
    "\n",
    "            for label, score, box in zip(labels, scores, boxes):\n",
    "                if score < 0.5:\n",
    "                    continue\n",
    "\n",
    "                class_name = COCO_INSTANCE_CATEGORY_NAMES[label]\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "\n",
    "                # Example behavior logic\n",
    "                if class_name == 'person':\n",
    "                    action = \"stop\"\n",
    "                elif class_name == 'knife':\n",
    "                    action = \"right\"\n",
    "\n",
    "            # Display action on screen\n",
    "            cv2.putText(img, f'Action: {action}', (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "            # Control robot movement\n",
    "            if action == \"stop\":\n",
    "                robot.stop()\n",
    "            elif action == \"right\":\n",
    "                robot.right(0.3)\n",
    "            else:\n",
    "                robot.forward(0.2)\n",
    "\n",
    "            # Display image\n",
    "            clear_output(wait=True)\n",
    "            display(PIL.Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        robot.stop()\n",
    "        camera.stop()\n",
    "        print(\"Stopped\")\n",
    "\n",
    "detect_and_act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fedcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842dc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "914c79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('logs'):\n",
    "    try:\n",
    "        os.remove('logs/'+i)\n",
    "    except IsADirectoryError:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7f6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(36):  # 10° steps for full rotation\n",
    "    robot.left(0.2)\n",
    "    time.sleep(0.1)\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ecef7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5db0d4df494b07826280a406867582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object monitor...\n",
      "Detected: shower cap\n",
      "Detected: plastic bag\n",
      "Detected: piggy bank\n",
      "Detected: Band Aid\n",
      "Detected: toilet seat\n",
      "Detected: oxygen mask\n",
      "Bottle detected! Moving forward.\n",
      "Detected: nipple\n",
      "Bottle detected! Moving forward.\n",
      "Stopping.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from jetbot import Robot, Camera\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "# Setup\n",
    "robot = Robot()\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "display(image)\n",
    "\n",
    "# Load ImageNet labels\n",
    "url = 'https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt'\n",
    "labels = urllib.request.urlopen(url).read().decode('utf-8').splitlines()\n",
    "\n",
    "\n",
    "# Load pretrained Faster R-CNN\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True).eval().cuda()\n",
    "\n",
    "# Load pretrained model\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Loop: check camera frame and act\n",
    "try:\n",
    "    print(\"Starting object monitor...\")\n",
    "    while True:\n",
    "        frame = camera.value\n",
    "        input_tensor = transform(frame).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            label = labels[predicted.item()]\n",
    "            print(f\"Detected: {label}\")\n",
    "\n",
    "            if label.lower() in [\"bottle\",\"switch\",\"nipple\",\"candle\",\"oxygen mask\"]:\n",
    "                print(\"Bottle detected! Moving forward.\")\n",
    "                robot.forward(0.3)\n",
    "            else:\n",
    "                robot.stop()\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping.\")\n",
    "    robot.stop()\n",
    "    camera.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894ca6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading module `ublox_gps`: No module named 'serial'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not initialize camera.  Please see error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetbot-0.4.3-py3.6.egg/jetbot/camera/opencv_gst_camera.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not read image from camera.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not read image from camera.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-15c9232eb648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# ——— SETUP ———\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mrobot\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mRobot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcamera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Show camera in notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetbot-0.4.3-py3.6.egg/jetbot/camera/opencv_gst_camera.py\u001b[0m in \u001b[0;36minstance\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mOpenCvGstCamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetbot-0.4.3-py3.6.egg/jetbot/camera/opencv_gst_camera.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             raise RuntimeError(\n\u001b[0;32m---> 37\u001b[0;31m                 'Could not initialize camera.  Please see error trace.')\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not initialize camera.  Please see error trace."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jetbot import Robot, Camera\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import traitlets, ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "from IPython.display import display\n",
    "\n",
    "# ——— CONFIG ———\n",
    "FORWARD_SPEED = 0.2\n",
    "LOOP_DELAY    = 0.2   # seconds between frames\n",
    "DETECT_THRESHOLD = 0.5\n",
    "\n",
    "# COCO class IDs we care about\n",
    "BOTTLE_CLASS    = 44\n",
    "STOP_CLASSES    = {1, 27, 33}   # person=1, backpack=27, suitcase=33\n",
    "\n",
    "\n",
    "# Load pretrained Faster R-CNN\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True).eval().cuda()\n",
    "\n",
    "# ——— SETUP ———\n",
    "robot  = Robot()\n",
    "camera = Camera.instance(width=640, height=480)\n",
    "\n",
    "# Show camera in notebook\n",
    "image_w = widgets.Image(format='jpeg', width=640, height=480)\n",
    "traitlets.dlink((camera, 'value'), (image_w, 'value'),\n",
    "                transform=bgr8_to_jpeg)\n",
    "display(image_w)\n",
    "\n",
    "# Preprocessing\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),      # scales to [0,1] and converts HWC→CHW\n",
    "])\n",
    "\n",
    "print(\"Starting object-detection loop (Ctrl+C to stop)…\")\n",
    "try:\n",
    "    while True:\n",
    "        frame = camera.value  # H×W×3 BGR uint8\n",
    "        img   = frame[:, :, ::-1]  # BGR→RGB\n",
    "        tensor = transform(img).cuda()\n",
    "\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model([tensor])[0]\n",
    "\n",
    "        # gather detections above threshold\n",
    "        labels = outputs['labels'].cpu().numpy()\n",
    "        scores = outputs['scores'].cpu().numpy()\n",
    "\n",
    "        # find highest-confidence bottle & stop-class detections\n",
    "        bottle_detected = any((labels[i]==BOTTLE_CLASS and scores[i]>=DETECT_THRESHOLD)\n",
    "                              for i in range(len(labels)))\n",
    "        stop_detected   = any((labels[i] in STOP_CLASSES and scores[i]>=DETECT_THRESHOLD)\n",
    "                              for i in range(len(labels)))\n",
    "\n",
    "        # act\n",
    "        if bottle_detected and not stop_detected:\n",
    "            robot.forward(FORWARD_SPEED)\n",
    "            print(\"→ Bottle only: MOVING forward\")\n",
    "        else:\n",
    "            robot.stop()\n",
    "            if stop_detected:\n",
    "                print(\"■ Stop-class detected → STOP\")\n",
    "            elif not bottle_detected:\n",
    "                print(\"■ No bottle detected → STOP\")\n",
    "            else:\n",
    "                print(\"■ Bottle + stop-class overlap → STOP\")\n",
    "\n",
    "        time.sleep(LOOP_DELAY)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    robot.stop()\n",
    "    camera.stop()\n",
    "    print(\"Stopped.\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31336bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting JetBot motion...\n",
      "\n",
      "--- Real-Time JetBot Metrics ---\n",
      "Time Elapsed: 0.03 s\n",
      "Estimated Distance: 0.01 m\n",
      "Speed: 0.25 m/s\n",
      "Momentum: 0.20 kg·m/s\n",
      "Acceleration: 8.64 m/s²\n",
      "Work Done: 0.05 J\n",
      "Kinetic Energy: 0.03 J\n",
      "Power Output: 1.73 W\n",
      "\n",
      "--- Real-Time JetBot Metrics ---\n",
      "Time Elapsed: 1.04 s\n",
      "Estimated Distance: 0.26 m\n",
      "Speed: 0.25 m/s\n",
      "Momentum: 0.20 kg·m/s\n",
      "Acceleration: 0.24 m/s²\n",
      "Work Done: 0.05 J\n",
      "Kinetic Energy: 0.03 J\n",
      "Power Output: 0.05 W\n",
      "\n",
      "--- Real-Time JetBot Metrics ---\n",
      "Time Elapsed: 2.04 s\n",
      "Estimated Distance: 0.51 m\n",
      "Speed: 0.25 m/s\n",
      "Momentum: 0.20 kg·m/s\n",
      "Acceleration: 0.12 m/s²\n",
      "Work Done: 0.05 J\n",
      "Kinetic Energy: 0.03 J\n",
      "Power Output: 0.02 W\n",
      "\n",
      "--- Real-Time JetBot Metrics ---\n",
      "Time Elapsed: 3.04 s\n",
      "Estimated Distance: 0.76 m\n",
      "Speed: 0.25 m/s\n",
      "Momentum: 0.20 kg·m/s\n",
      "Acceleration: 0.08 m/s²\n",
      "Work Done: 0.05 J\n",
      "Kinetic Energy: 0.03 J\n",
      "Power Output: 0.02 W\n",
      "\n",
      "--- Real-Time JetBot Metrics ---\n",
      "Time Elapsed: 4.04 s\n",
      "Estimated Distance: 1.01 m\n",
      "Speed: 0.25 m/s\n",
      "Momentum: 0.20 kg·m/s\n",
      "Acceleration: 0.06 m/s²\n",
      "Work Done: 0.05 J\n",
      "Kinetic Energy: 0.03 J\n",
      "Power Output: 0.01 W\n",
      "\n",
      "JetBot stopped.\n"
     ]
    }
   ],
   "source": [
    "# Real-time physics metrics for JetBot motion\n",
    "import time\n",
    "import math\n",
    "from jetbot import Robot\n",
    "\n",
    "# Constants for Waveshare JetBot\n",
    "WEIGHT_KG = 0.8\n",
    "BATTERY_VOLTAGE = 7.4  # volts\n",
    "BATTERY_CAPACITY_MAH = 7800\n",
    "BATTERY_CAPACITY_WH = (BATTERY_VOLTAGE * BATTERY_CAPACITY_MAH) / 1000  # watt-hours\n",
    "BATTERY_CAPACITY_J = BATTERY_CAPACITY_WH * 3600  # convert to joules\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "# Initialize logging variables\n",
    "start_time = time.time()\n",
    "robot.forward(0.3)  # Move forward at 30% speed\n",
    "print(\"Starting JetBot motion...\")\n",
    "\n",
    "# Real-time tracking loop\n",
    "while True:\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    if elapsed_time >= 5.0:  # Move for 5 seconds\n",
    "        break\n",
    "\n",
    "    # Simulated distance assuming speed 0.25 m/s\n",
    "    speed = 0.25  # m/s\n",
    "    distance = speed * elapsed_time  # m\n",
    "\n",
    "    # Basic physics metrics\n",
    "    acceleration = speed / elapsed_time  # m/s^2\n",
    "    momentum = WEIGHT_KG * speed  # kg.m/s\n",
    "    kinetic_energy = 0.5 * WEIGHT_KG * speed ** 2  # J\n",
    "    work_done = WEIGHT_KG * acceleration * distance  # J\n",
    "    power_output = work_done / elapsed_time if elapsed_time > 0 else 0  # W\n",
    "\n",
    "    # Display metrics in real-time\n",
    "    print(\"\\n--- Real-Time JetBot Metrics ---\")\n",
    "    print(f\"Time Elapsed: {elapsed_time:.2f} s\")\n",
    "    print(f\"Estimated Distance: {distance:.2f} m\")\n",
    "    print(f\"Speed: {speed:.2f} m/s\")\n",
    "    print(f\"Momentum: {momentum:.2f} kg·m/s\")\n",
    "    print(f\"Acceleration: {acceleration:.2f} m/s²\")\n",
    "    print(f\"Work Done: {work_done:.2f} J\")\n",
    "    print(f\"Kinetic Energy: {kinetic_energy:.2f} J\")\n",
    "    print(f\"Power Output: {power_output:.2f} W\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Stop JetBot\n",
    "robot.stop()\n",
    "print(\"\\nJetBot stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
