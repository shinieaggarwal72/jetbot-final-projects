{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Follower - Train Model\n",
    "\n",
    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
    "\n",
    "We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data\n",
    "\n",
    "Before you start, you should upload the ``road_following_<Date&Time>.zip`` file that you created in the ``data_collection.ipynb`` notebook on the robot. \n",
    "\n",
    "> If you're training on the JetBot you collected data on, you can skip this!\n",
    "\n",
    "You should then extract this dataset by calling the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a8e43abf5c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip -q road_following.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 304\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "!unzip -q road_following.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset_all`` appear in the file browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance\n",
    "\n",
    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
    "we can use all of the torch data utilities :)\n",
    "\n",
    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid/corrupt image: dataset_xy/xy_093_148_9e50747e-3538-11f0-8ad3-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_093_148_9e50747e-3538-11f0-8ad3-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_155_143_20b08a44-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_155_143_20b08a44-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_108_183_99213e70-3538-11f0-8ad3-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_108_183_99213e70-3538-11f0-8ad3-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_112_150_db5c4804-3536-11f0-b666-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_112_150_db5c4804-3536-11f0-b666-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_128_144_7f8c7ea0-353a-11f0-ac47-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_128_144_7f8c7ea0-353a-11f0-ac47-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_141_175_602e564a-353b-11f0-919b-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_141_175_602e564a-353b-11f0-919b-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_131_153_81be791c-353a-11f0-ac47-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_131_153_81be791c-353a-11f0-ac47-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_132_099_25b9b74a-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_132_099_25b9b74a-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_104_065_14b5ed5e-3536-11f0-9123-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_104_065_14b5ed5e-3536-11f0-9123-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_107_085_86043016-353a-11f0-ac47-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_107_085_86043016-353a-11f0-ac47-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_133_164_680ff864-353b-11f0-919b-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_133_164_680ff864-353b-11f0-919b-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_110_190_6b9789ac-353b-11f0-919b-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_110_190_6b9789ac-353b-11f0-919b-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_158_177_189cc016-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_158_177_189cc016-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_121_109_274f7ffe-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_121_109_274f7ffe-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_135_149_13c82418-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_135_149_13c82418-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_048_160_d4362a40-3536-11f0-b666-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_048_160_d4362a40-3536-11f0-b666-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_135_156_970ac066-3538-11f0-8ad3-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_135_156_970ac066-3538-11f0-8ad3-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_110_083_dec36918-3537-11f0-b56c-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_110_083_dec36918-3537-11f0-b56c-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_138_174_5c6ea12c-353b-11f0-919b-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_138_174_5c6ea12c-353b-11f0-919b-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_100_070_83b6bc3e-353a-11f0-ac47-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_100_070_83b6bc3e-353a-11f0-ac47-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_068_146_d858990e-3537-11f0-b56c-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_068_146_d858990e-3537-11f0-b56c-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_140_141_dea349ea-3536-11f0-b666-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_140_141_dea349ea-3536-11f0-b666-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_054_110_d7b9c122-3536-11f0-b666-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_054_110_d7b9c122-3536-11f0-b666-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_160_182_64c8ddd8-353b-11f0-919b-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_160_182_64c8ddd8-353b-11f0-919b-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_105_127_db8652ba-3537-11f0-b56c-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_105_127_db8652ba-3537-11f0-b56c-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_141_177_168022b4-3539-11f0-82a2-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_141_177_168022b4-3539-11f0-82a2-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_128_178_9bddf0ea-3538-11f0-8ad3-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_128_178_9bddf0ea-3538-11f0-8ad3-e0e1a91dc0f8.jpg'\n",
      "Skipping invalid/corrupt image: dataset_xy/xy_038_177_d0f78176-3536-11f0-b666-e0e1a91dc0f8.jpg - cannot identify image file 'dataset_xy/xy_038_177_d0f78176-3536-11f0-b666-e0e1a91dc0f8.jpg'\n"
     ]
    }
   ],
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "        \n",
    "        all_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.image_paths = []\n",
    "        for path in all_paths:\n",
    "            try:\n",
    "                with PIL.Image.open(path) as img:\n",
    "                    img.verify()  # Check if image is readable\n",
    "                self.image_paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid/corrupt image: {path} - {e}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_xy', random_hflips=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets\n",
    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches\n",
    "\n",
    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model \n",
    "\n",
    "We use ResNet-18 model available on PyTorch TorchVision. \n",
    "\n",
    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "\n",
    "\n",
    "More details on ResNet-18 : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 1\n",
    "\n",
    "Finally, we transfer our model for execution on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Regression:\n",
    "\n",
    "We train for 50 epochs and save best model if the loss is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /media/nvidia/NVME/pytorch/pytorch-v1.9.0/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.099774, 5.333559\n",
      "0.588573, 15.411853\n",
      "0.114549, 0.392215\n",
      "0.082219, 0.132861\n",
      "0.064815, 0.093167\n",
      "0.059095, 0.088969\n",
      "0.049616, 0.041250\n",
      "0.051873, 0.061752\n",
      "0.055969, 0.053755\n",
      "0.052253, 0.087857\n",
      "0.037609, 0.021735\n",
      "0.035707, 0.042695\n",
      "0.047334, 0.042250\n",
      "0.075989, 0.039624\n",
      "0.051234, 0.027645\n",
      "0.048626, 0.040291\n",
      "0.027742, 0.097265\n",
      "0.022839, 0.044729\n",
      "0.024555, 0.037570\n",
      "0.048993, 0.052573\n",
      "0.043731, 0.033796\n",
      "0.052157, 0.028080\n",
      "0.032506, 0.056402\n",
      "0.025617, 0.042925\n",
      "0.015814, 0.084032\n",
      "0.020236, 0.034118\n",
      "0.038712, 0.042909\n",
      "0.023673, 0.040444\n",
      "0.033450, 0.173921\n",
      "0.027493, 0.047875\n",
      "0.024042, 0.049544\n",
      "0.028454, 0.057552\n",
      "0.030959, 0.023860\n",
      "0.028496, 0.040926\n",
      "0.019345, 0.037239\n",
      "0.052163, 0.072557\n",
      "0.042400, 0.039446\n",
      "0.028579, 0.041119\n",
      "0.012478, 0.049265\n",
      "0.021038, 0.042176\n",
      "0.015329, 0.043807\n",
      "0.026978, 0.057919\n",
      "0.037509, 0.064560\n",
      "0.017938, 0.036735\n",
      "0.026372, 0.073676\n",
      "0.037456, 0.065856\n",
      "0.018497, 0.078107\n",
      "0.017290, 0.052700\n",
      "0.029310, 0.067752\n",
      "0.027818, 0.053409\n",
      "0.016914, 0.033105\n",
      "0.018783, 0.048073\n",
      "0.010189, 0.047008\n",
      "0.037546, 0.038256\n",
      "0.034040, 0.125539\n",
      "0.030372, 0.043540\n",
      "0.029032, 0.027616\n",
      "0.026230, 0.035098\n",
      "0.027184, 0.032493\n",
      "0.019229, 0.056382\n",
      "0.017269, 0.044488\n",
      "0.014826, 0.047273\n",
      "0.013836, 0.045729\n",
      "0.014271, 0.084058\n",
      "0.014905, 0.070327\n",
      "0.014397, 0.039017\n",
      "0.010479, 0.055223\n",
      "0.017605, 0.060822\n",
      "0.012409, 0.044476\n",
      "0.011032, 0.053047\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 70\n",
    "BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('%f, %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it will generate ``best_steering_model_xy.pth`` file which you can use for inferencing in the live demo notebook.\n",
    "\n",
    "If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
